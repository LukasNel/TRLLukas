# DeepTools

DeepTools is a powerful Python library that helps integrate tool calling into reasoning traces, built on top of the smolagents local Python interpreter. It provides a flexible framework for creating AI agents from reasoning agents that can use tools and perform complex reasoning tasks.

## Features

- **Tool Integration**: Seamlessly integrate custom tools into AI agent reasoning
- **Multiple Model Support**: Built-in support for both LiteLLM and vLLM models
- **Flexible Sampling**: Abstract sampler interface for custom model implementations
- **Secure Execution**: Controlled Python environment with authorized imports
- **Streaming Support**: Asynchronous generation with streaming responses
- **Extensible Architecture**: Easy to add new tools and samplers

## Installation

```bash
pip install deeptools
```

## Quick Start

```python
from deeptools.toolcaller import ToolCaller
from deeptools.samplers import LiteLLMSampler
from deeptools.tools.yfinance_tools import StockPriceTool
from datetime import datetime

# Initialize the tool caller with a LiteLLM model
toolcaller = ToolCaller(
    sampler=LiteLLMSampler(model_name="gpt-3.5-turbo"),
    authorized_imports=["pandas"]
)

# Add tools
tools = [StockPriceTool(cutoff_date=datetime.now().strftime("%Y-%m-%d"))]

# Generate responses
async for output in toolcaller.generate(
    user_prompt="What was Apple's stock price last week?",
    system_prompt="You are a helpful AI assistant...",
    tools=tools
):
    print(output, end="")
```

## Architecture

### ToolCaller

The `ToolCaller` class is the main interface for integrating tools with language models. It manages the execution environment, tool registration, and interaction between the language model and tools.

#### Initialization Parameters

The ToolCaller constructor takes the following parameters:

```python
def __init__(self, sampler: AbstractSampler, authorized_imports: list[str] = []):
```

- `sampler` (required): An instance of `AbstractSampler` that handles model inference. This can be either:
  - `LiteLLMSampler` for cloud-based models (e.g., GPT-3.5-turbo)
  - `VLLMSampler` for local models (e.g., Qwen/Qwen2.5-7B)
- `authorized_imports` (optional): A list of Python package names that are allowed to be imported in the execution environment. Defaults to an empty list. This provides security by restricting which packages can be used in code execution.

Example initialization:
```python
toolcaller = ToolCaller(
    sampler=LiteLLMSampler(model_name="gpt-3.5-turbo"),
    authorized_imports=["pandas", "numpy", "requests"]
)
```

The ToolCaller delegates the actual model inference to the provided sampler while managing the tool execution environment. It:

1. Uses the sampler to generate model responses
2. Manages the conversation flow and tool execution
3. Handles the secure Python environment for code execution

The ToolCaller provides two main methods:

1. `generate(system_prompt: str, user_prompt: str, tools: list[Tool] = [])`:
   - Takes a system prompt (must include `{tool_desc}` placeholder), user prompt, and optional list of tools
   - Initializes a Python execution environment with authorized imports
   - Registers the provided tools and generates their descriptions
   - Creates a message history starting with system and user prompts
   - Uses the sampler to generate model responses
   - Streams the model's response, handling code blocks and tool execution
   - Returns an async generator yielding both model outputs and tool execution results


The ToolCaller works by:
1. Taking a system prompt that includes a `{tool_desc}` placeholder
2. Automatically replacing `{tool_desc}` with formatted descriptions of all registered tools
3. Using the provided sampler to generate model responses
4. Managing the conversation flow between the model and tools
5. Executing code blocks in a controlled environment
6. Streaming both model outputs and tool execution results

Example of how the ToolCaller processes a request:
```python
# Initialize the ToolCaller with a specific sampler
toolcaller = ToolCaller(
    # Choose either LiteLLMSampler for cloud models or VLLMSampler for local models
    sampler=LiteLLMSampler(model_name="gpt-3.5-turbo"),  # or VLLMSampler(model_id="Qwen/Qwen2.5-7B")
    # Specify which Python packages can be imported in the execution environment
    authorized_imports=["pandas", "numpy"]
)

# Define tools
tools = [StockPriceTool(cutoff_date="2024-03-20")]

# Generate responses
async for output in toolcaller.generate(
    system_prompt="You are an expert assistant. Available tools: {tool_desc}",
    user_prompt="What was Apple's stock price last week?",
    tools=tools
):
    # Output can be:
    # 1. Model's text response (generated by the sampler)
    # 2. Tool execution results
    # 3. Code block execution logs
    print(output, end="")
```

The ToolCaller handles several key aspects:
- **Model Integration**: Delegates model inference to the provided sampler
- **Tool Registration**: Automatically registers tools and generates their descriptions
- **Code Execution**: Manages a secure Python environment for executing code blocks
- **Streaming**: Provides streaming responses for both model outputs and tool results
- **Error Handling**: Captures and reports errors from both model and tool execution
- **State Management**: Maintains conversation history and execution state

### Samplers

DeepTools provides an abstract sampler interface and two concrete implementations:

1. **AbstractSampler**: Base class defining the sampling interface
2. **LiteLLMSampler**: Integration with LiteLLM for various model providers
3. **VLLMSampler**: High-performance inference using vLLM

The samplers expect messages to follow the OpenAI chat completion format, where each message is a dictionary with:
- `role`: One of "system", "user", or "assistant"
- `content`: The message content as a string

Example message format:
```python
messages = [
    {"role": "system", "content": "You are a helpful assistant..."},
    {"role": "user", "content": "What's the weather like?"},
    {"role": "assistant", "content": "Let me check that for you..."}
]
```

To create a custom sampler:

```python
from deeptools.samplers.abstract import AbstractSampler
from typing import AsyncGenerator

class CustomSampler(AbstractSampler):
    async def sample(self, messages: list[dict[str, str]]) -> AsyncGenerator[str, None]:
        # messages will be a list of dictionaries following the OpenAI format
        # Implement your sampling logic here
        yield "response"
```

### Tools

Tools are implemented using the `Tool` class from smolagents. Each tool must define:

- `name`: Unique identifier for the tool
- `description`: Documentation of the tool's purpose
- `inputs`: Dictionary of input parameters and their types
- `output_type`: Type of the tool's output
- `forward()`: Implementation of the tool's functionality

Example tool implementation:

```python
from smolagents import Tool

class CustomTool(Tool):
    name = "custom_tool"
    description = "Description of what the tool does"
    inputs = {
        "param1": {
            "type": "string",
            "description": "Description of param1"
        }
    }
    output_type = "object"

    def forward(self, param1: str):
        # Implement tool logic here
        return result
```

## Adding New Tools

To add a new tool:

1. Implement your tool class inheriting from `Tool`
2. Define the required attributes (name, description, inputs, output_type)
3. Implement the `forward()` method
4. Import and use your tool with the `ToolCaller`

Example:

```python
from smolagents import Tool

class MyCustomTool(Tool):
    name = "my_custom_tool"
    description = "A custom tool that does something useful"
    inputs = {
        "input_param": {
            "type": "string",
            "description": "Input parameter description"
        }
    }
    output_type = "object"

    def forward(self, input_param: str):
        # Tool implementation
        return {"result": f"Processed {input_param}"}
```

## Authorized Imports

The `ToolCaller` allows you to specify which Python packages can be imported in the execution environment. This provides security and control over what code can be executed.

```python
toolcaller = ToolCaller(
    sampler=LiteLLMSampler(model_name="gpt-3.5-turbo"),
    authorized_imports=["pandas", "numpy", "requests"]
)
```

## Best Practices

1. **Tool Design**:
   - Keep tools focused and single-purpose
   - Provide clear descriptions and parameter documentation
   - Handle errors gracefully
   - Use type hints for better code clarity

2. **Security**:
   - Only authorize necessary imports
   - Validate tool inputs
   - Implement appropriate access controls
   - Use cutoff dates for time-sensitive data

3. **Performance**:
   - Use streaming for long-running operations
   - Implement caching where appropriate
   - Consider using vLLM for high-performance inference

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## License

This project is licensed under the MIT license.

## Examples

### Using LiteLLM ToolCaller

Here's a complete example of using the LiteLLM ToolCaller with a GPT model:

```python
from deeptools.toolcaller import ToolCaller
from deeptools.samplers import LiteLLMSampler
from deeptools.tools.yfinance_tools import StockPriceTool
from datetime import datetime
import asyncio

async def main():
    # Initialize the LiteLLM toolcaller
    litellm_toolcaller = ToolCaller(
        sampler=LiteLLMSampler(model_name="gpt-3.5-turbo"),
        authorized_imports=["pandas"]
    )
    
    # Define the system prompt with tool descriptions
    # Note: The {tool_desc} placeholder is required and will be replaced with tool descriptions
    system_prompt = """You are an expert assistant. You will be given a task to solve as best you can. 
    You have access to a python interpreter and a set of tools that runs anything you write in a code block.
    You have access to pandas. 
    All code blocks written between ```python and ``` will get executed by a python interpreter and the result will be given to you.
    On top of performing computations in the Python code snippets that you create, you only have access to these tools:
    {tool_desc}
    """
    
    # Add tools with a cutoff date for data access
    tools = [StockPriceTool(cutoff_date=datetime.now().strftime("%Y-%m-%d"))]
    
    # Generate responses
    async for output in litellm_toolcaller.generate(
        user_prompt="What was Apple's stock price last week?",
        system_prompt=system_prompt,  # The {tool_desc} will be automatically replaced
        tools=tools
    ):
        print(output, end="")

if __name__ == "__main__":
    asyncio.run(main())
```

### Using vLLM ToolCaller

Here's a complete example of using the vLLM ToolCaller for high-performance inference:

```python
from deeptools.toolcaller import ToolCaller
from deeptools.samplers import VLLMSampler
from deeptools.tools.yfinance_tools import StockPriceTool, CompanyFinancialsTool
from datetime import datetime
import asyncio

async def main():
    # Initialize the vLLM toolcaller with a local model
    vllm_toolcaller = ToolCaller(
        sampler=VLLMSampler(model_id="Qwen/Qwen2.5-7B"),  # or any other model supported by vLLM
        authorized_imports=["pandas"]
    )
    
    # Define the system prompt
    # Note: The {tool_desc} placeholder is required and will be replaced with tool descriptions
    system_prompt = """You are an expert assistant. You will be given a task to solve as best you can. 
    You have access to a python interpreter and a set of tools that runs anything you write in a code block.
    You have access to pandas. 
    All code blocks written between ```python and ``` will get executed by a python interpreter and the result will be given to you.
    On top of performing computations in the Python code snippets that you create, you only have access to these tools:
    {tool_desc}
    """
    
    # Add multiple tools
    tools = [
        StockPriceTool(cutoff_date=datetime.now().strftime("%Y-%m-%d")),
        CompanyFinancialsTool(cutoff_date=datetime.now().strftime("%Y-%m-%d"))
    ]
    
    # Generate responses
    async for output in vllm_toolcaller.generate(
        user_prompt="Analyze Apple's financial performance over the last quarter",
        system_prompt=system_prompt,  # The {tool_desc} will be automatically replaced
        tools=tools
    ):
        print(output, end="")

if __name__ == "__main__":
    asyncio.run(main())
```


